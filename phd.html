<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Research Questions – Anna-Maria Christodoulou</title>

  <!-- Thebe for interactive Python cells -->
<script src="https://unpkg.com/thebe@latest/lib/index.js"></script>
<link rel="stylesheet" href="https://unpkg.com/thebe@latest/lib/thebe.css" />

  <style>
    body {
      font-family: "Trebuchet MS", sans-serif;
      background-color: #9ee0f0;
      margin: 0;
      padding: 0;
    }
    header {
      text-align: center;
      padding: 25px;
      background-color: #005c7a;
      color: #fff;
    }
    nav {
      background-color: #003d50;
      text-align: center;
      padding: 12px 0;
    }
    nav a {
      color: white;
      margin: 0 15px;
      text-decoration: none;
      font-size: 1.1rem;
      font-weight: bold;
    }
    nav a:hover {
      text-decoration: underline;
    }
    .container {
      max-width: 900px;
      margin: 30px auto;
      background: #ffffffdd;
      padding: 25px;
      border-radius: 12px;
    }
    h2 {
      color: #003d50;
    }
    p {
      line-height: 1.6;
    }

    /* --- PLAYFUL RESEARCH MAP --- */
    .map-container {
      margin-top: 35px;
      padding: 20px;
      background: #e7faff;
      border-radius: 12px;
      border: 2px dashed #00789e;
    }
    .map-title {
      text-align: center;
      font-size: 1.4rem;
      margin-bottom: 15px;
      color: #005c7a;
      font-weight: bold;
    }
    .map {
      display: flex;
      flex-direction: column;
      gap: 20px;
    }
    .map-row {
      display: flex;
      justify-content: center;
      gap: 20px;
      flex-wrap: wrap;
    }
    .bubble {
      background: #fff;
      border-radius: 20px;
      padding: 15px 20px;
      min-width: 180px;
      text-align: center;
      border: 3px solid #00a4c8;
      font-weight: bold;
      transition: transform 0.2s;
    }
    .bubble:hover {
      transform: scale(1.06);
      background: #dafeff;
    }
    .arrow {
      text-align: center;
      font-size: 2rem;
      color: #005c7a;
    }

    /* INTERACTIVE DESCRIPTION BOX */
#info-box {
display: none;
margin-top: 20px;
padding: 15px;
border-radius: 10px;
background: #ffffffcc;
border: 2px solid #00789e;
}


/* INTERACTIVE CODE BLOCKS */
.cell-block {
background: #f4f4f4;
padding: 15px;
margin: 20px 0;
border-radius: 8px;
border: 1px solid #cccccc;
}

  </style>
</head>

<body>
  <header>
    <h1>Research Questions</h1>
    <p>An introduction to my PhD project</p>
  </header>

  <nav>
    <a href="index.html">Home</a>
    <a href="about.html">About</a>
    <a href="research.html">Research</a>
  </nav>

  <div class="container">

    <h2>What My PhD Is About</h2>
    <p>
      My PhD examines how computers can better understand music by looking at
      more than just audio. I do that by looking at how we humans understand music by not only
      listening to it but also moving to the beat, looking at different visuals, reading text/lyrics,
      understanding cultural cues, and monitoring the physical behavior of performers. This multidimensional 
      integration is called <strong>multimodal perception</strong>. When we combine recordings of multiple data
      sources in a computational system, we call it <strong>multimodal fusion</strong>, and it is a way to model music in a
      deeper and more human-like manner
    </p>

    <p>
      This page explains the main research questions in my project,
      and gives you a visual “map” of how the ideas connect.
    </p>

    <!-- PLAYFUL GRAPHICAL MAP -->
    <div class="map-container">
      <div class="map-title">Multimodal Music Research Map</div>
      <div class="map">

        <div class="map-row">
          <div class="bubble">Music is Multimodal</div>
          <div class="bubble">We Use Data to Study Music</div>
          <div class="bubble">Technology Supports Understanding</div>
        </div>

        <div class="arrow">⬇️</div>

        <div class="map-row">
          <div class="bubble">RQ1: What Is multimodality in music processing?</div>
          <div class="bubble">RQ2: To which extent can technology enhance the contextual understanding of music?</div>
        </div>

        <div class="arrow">⬇️</div>

        <div class="map-row">
          <div class="bubble">Definitions & Frameworks</div>
          <div class="bubble">Audio-Video-Motion Datasets</div>
          <div class="bubble">Music Question Answering</div>
          <div class="bubble">Ethics & Data Management</div>
        </div>

        <div class="arrow">⬇️</div>

        <div class="map-row">
          <div class="bubble" style="background:#c8ffe0;">Better Understanding of Music</div>
        </div>

      </div>
    </div>

    <h2>Research Question 1</h2>
    <p>
      <strong>How can multimodality be defined and used in computational music analysis?</strong>
    </p>
    <p>
      Different fields talk about multimodality in different ways. In my PhD,
      I propose a simple idea:
      <em>use multiple data sources only when each one adds something new and meaningful to the task.</em>
      This helps researchers choose the right combination of audio, video,
      motion, text, and metadata, depending on what they want to understand.
    </p>

    <h2>Research Question 2</h2>
    <p>
      <strong>How much can technology enhance the contextual understanding of music?</strong>
    </p>
    <p>
      I worked with several multimodal datasets, such as folk music, classical music,
      ensemble performance, motion capture, lyrics, and metadata, to see
      whether machines can answer questions like:
    </p>
    <ul>
      <li>What is happening in the performance?</li>
      <li>Why is the performer moving this way?</li>
      <li>How does the audience react at certain points during the performance?</li>
    </ul>

    <p>
      Before we dive into some details, perhaps you would like to know:

      <p>
        <strong>What is Music Performance?</strong>
      </p>
      <p>Let's just say that music performance is music in action. It’s when performers turn compositions, improvisations, or musical ideas into sound, movement, and 
        expression for an audience. Think of it as storytelling with sound, gestures, and emotion.</p>
      <p>
        <strong>What is Music Performance Context?</strong>
      </p>
    <p>Context is everything around the music that shapes its meaning: the venue, the audience, the gestures of performers, cultural rules, and even the placement of 
      instruments. It’s what makes the same piece of music feel different in a concert hall, a jazz club, or a folk festival.</p>
      <p><strong>What is Music Understanding?</strong></p>
    <p>Music understanding begins when we stop simply hearing sound around us and start listening with attention. 
      Hearing is passive and happens automatically. Listening is active and happens when we focus, follow, feel, and make sense of what unfolds in the music. 
      Understanding music isn’t about knowing fancy terminology or analyzing scores, but about noticing what the music is doing, how it moves, how it feels, and how our
      own experiences help us interpret it.</p>
    <p>People often imagine that only trained musicians or musicologists can “understand” music, but everyone can! A person may not know what a motif or cadence is, 
      but they can still feel repetition, arrival, contrast, or surprise. They can follow a musical idea as it evolves, or sense the energy in a performer’s gesture. 
      Understanding music is less about naming things and more about experiencing how music flows and how it speaks.
</p>
<div class="tree">
  <ul>
    <li>
      <strong>Music Understanding</strong>
      <ul>
        <li>
          <strong>Listening (not just hearing!)</strong>
          <ul>
            <li>Attention + curiosity</li>
          </ul>
        </li>
        <li>
          <strong>Perception</strong>
          <ul>
            <li>How we group, follow, and sense musical ideas</li>
          </ul>
        </li>
        <li>
          <strong>Cognition</strong>
          <ul>
            <li>Memory, prediction, timing, reasoning</li>
          </ul>
        </li>
        <li>
          <strong>Embodiment</strong>
          <ul>
            <li>How our body feels, moves, and simulates the music</li>
          </ul>
        </li>
        <li>
          <strong>Emotion &amp; Experience</strong>
          <ul>
            <li>What the music makes us feel and imagine</li>
          </ul>
        </li>
        <li>
          <strong>Culture</strong>
          <ul>
            <li>What our background teaches us to notice and value</li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</div>

<style>
/* simple tree lines */
.tree ul {
  padding-left: 1.2rem;
  position: relative;
  list-style: none;
  margin: 0;
}
.tree ul::before {
  content: "";
  position: absolute;
  top: 0;
  left: 0.55rem;            /* adjust to align vertical line */
  bottom: 0;
  width: 1px;
  background: #cfdbe0;      /* vertical connector color */
}
.tree li {
  margin: 0.5rem 0;
  padding-left: 1rem;
  position: relative;
}
.tree li::before {
  content: "";
  position: absolute;
  left: 0;
  top: 0.9rem;              /* position horizontal connector */
  width: 0.6rem;
  height: 1px;
  background: #cfdbe0;
}
.tree strong { color: #003d50; }
.tree li ul::before { left: 0.35rem; } /* nested alignment */
</style>
    <p><strong>What is music information processing?</strong></p>
    <p>Computational music information processing is about turning the physical experience of music into digital data, and then into forms of machine cognition, so that
      computers can help us explore, analyze, and understand music in new ways.</p>

    <div class="cell-block thebe-cell" data-executable="true" data-language="python">
<pre data-executable="true" data-language="python">
metadata = {
"tempo": 95,
"key": "A minor",
"instrumentation": ["violin", "laouto", "voice"],
}


question = "What is the key of the piece?"


if "key" in question.lower():
answer = metadata["key"]
else:
answer = "Not sure yet!"


answer
</pre>
</div>


</div>


<script>
function showInfo(text) {
const box = document.getElementById("info-box");
box.innerHTML = text;
box.style.display = "block";
}


thebe.config = {
useBinder: true,
binderOptions: {
repo: "binder-examples/jupyter-stacks-datascience",
ref: "master",
}
};
thebe.bootstrap();
</script>
</body>
</html>
    

