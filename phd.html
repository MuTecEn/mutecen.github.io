<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Research Questions ‚Äì Anna-Maria Christodoulou</title>

  <!-- Thebe for interactive Python cells -->
  <script src="https://unpkg.com/thebe@latest/lib/index.js"></script>
  <link rel="stylesheet" href="https://unpkg.com/thebe@latest/lib/thebe.css" />

  <style>
    body {
      font-family: "Trebuchet MS", sans-serif;
      background-color: #9ee0f0;
      margin: 0;
      padding: 0;
    }
    header {
      text-align: center;
      padding: 25px;
      background-color: #005c7a;
      color: #fff;
    }
    nav {
      background-color: #003d50;
      text-align: center;
      padding: 12px 0;
    }
    nav a {
      color: white;
      margin: 0 15px;
      text-decoration: none;
      font-size: 1.1rem;
      font-weight: bold;
    }
    nav a:hover {
      text-decoration: underline;
    }
    .container {
      max-width: 900px;
      margin: 30px auto;
      background: #ffffffdd;
      padding: 25px;
      border-radius: 12px;
    }
    h2 {
      color: #003d50;
    }
    p {
      line-height: 1.6;
    }

    /* --- PLAYFUL RESEARCH MAP --- */
    .map-container {
      margin-top: 35px;
      padding: 20px;
      background: #e7faff;
      border-radius: 12px;
      border: 2px dashed #00789e;
    }
    .map-title {
      text-align: center;
      font-size: 1.4rem;
      margin-bottom: 15px;
      color: #005c7a;
      font-weight: bold;
    }
    .map {
      display: flex;
      flex-direction: column;
      gap: 20px;
    }
    .map-row {
      display: flex;
      justify-content: center;
      gap: 20px;
      flex-wrap: wrap;
    }
    .bubble {
      background: #fff;
      border-radius: 20px;
      padding: 15px 20px;
      min-width: 180px;
      text-align: center;
      border: 3px solid #00a4c8;
      font-weight: bold;
      transition: transform 0.2s;
    }
    .bubble:hover {
      transform: scale(1.06);
      background: #dafeff;
    }
    .arrow {
      text-align: center;
      font-size: 2rem;
      color: #005c7a;
    }

    /* TREE */
    .tree ul {
      padding-left: 1.2rem;
      position: relative;
      list-style: none;
      margin: 0;
    }
    .tree ul::before {
      content: "";
      position: absolute;
      top: 0;
      left: 0.55rem;
      bottom: 0;
      width: 1px;
      background: #cfdbe0;
    }
    .tree li {
      margin: 0.5rem 0;
      padding-left: 1rem;
      position: relative;
    }
    .tree li::before {
      content: "";
      position: absolute;
      left: 0;
      top: 0.9rem;
      width: 0.6rem;
      height: 1px;
      background: #cfdbe0;
    }
    .tree strong { color: #003d50; }
    .tree li ul::before { left: 0.35rem; }

    /* INTERACTIVE CELL BLOCK */
    .cell-block {
      background: #f4f4f4;
      padding: 15px;
      margin: 20px 0;
      border-radius: 8px;
      border: 1px solid #cccccc;
    }

    /* INTERACTIVE MULTIMODALITY TEST */
    .multimodal-test {
      background: #e7faff;
      padding: 20px;
      border-radius: 12px;
      border: 2px dashed #00789e;
      margin-top: 25px;
    }
    .multimodal-test select, .multimodal-test label {
      display: block;
      margin: 8px 0;
    }
    .multimodal-test button {
      margin-top: 10px;
      padding: 8px 12px;
      border: none;
      border-radius: 5px;
      background: #00a4c8;
      color: #fff;
      font-weight: bold;
      cursor: pointer;
      transition: 0.2s;
    }
    .multimodal-test button:hover {
      background: #00789e;
    }
    .multimodal-test #result {
      margin-top: 12px;
      font-weight: bold;
    }
  </style>
</head>

<body>
<header>
  <h1>Research Questions</h1>
  <p>An introduction to my PhD project</p>
</header>

<nav>
  <a href="index.html">Home</a>
  <a href="about.html">About</a>
  <a href="research.html">Research</a>
</nav>

<div class="container">

<h2>What My PhD Is About</h2>
<p>
  My PhD examines how computers can better understand music by looking at
  more than just audio. I do that by looking at how we humans understand music by not only
  listening to it but also moving to the beat, looking at different visuals, reading text/lyrics,
  understanding cultural cues, and monitoring the physical behavior of performers. This multidimensional 
  integration is called <strong>multimodal perception</strong>. When we combine recordings of multiple data
  sources in a computational system, we call it <strong>multimodal fusion</strong>, and it is a way to model music in a
  deeper and more human-like manner
</p>

<p>
  This page explains the main research questions in my project,
  and gives you a visual ‚Äúmap‚Äù of how the ideas connect.
</p>

<!-- PLAYFUL GRAPHICAL MAP -->
<div class="map-container">
  <div class="map-title">Multimodal Music Research Map</div>
  <div class="map">
    <div class="map-row">
      <div class="bubble">Music is Multimodal</div>
      <div class="bubble">We Use Data to Study Music</div>
      <div class="bubble">Technology Supports Understanding</div>
    </div>
    <div class="arrow">‚¨áÔ∏è</div>
    <div class="map-row">
      <div class="bubble">RQ1: What Is multimodality in music processing?</div>
      <div class="bubble">RQ2: To which extent can technology enhance the contextual understanding of music?</div>
    </div>
    <div class="arrow">‚¨áÔ∏è</div>
    <div class="map-row">
      <div class="bubble">Definitions & Frameworks</div>
      <div class="bubble">Audio-Video-Motion Datasets</div>
      <div class="bubble">Music Question Answering</div>
      <div class="bubble">Ethics & Data Management</div>
    </div>
    <div class="arrow">‚¨áÔ∏è</div>
    <div class="map-row">
      <div class="bubble" style="background:#c8ffe0;">Better Understanding of Music</div>
    </div>
  </div>
</div>

<h2>Research Question 1</h2>
<p>
  <strong>How can multimodality be defined and used in computational music analysis?</strong>
</p>
<p>
  Different fields talk about multimodality in different ways. In my PhD,
  I propose a simple idea:
  <em>use multiple data sources only when each one adds something new and meaningful to the task.</em>
  This helps researchers choose the right combination of audio, video,
  motion, text, and metadata, depending on what they want to understand.
</p>

<h2>Research Question 2</h2>
<p>
  <strong>How much can technology enhance the contextual understanding of music?</strong>
</p>
<p>
  I worked with several multimodal datasets, such as folk music, classical music,
  ensemble performance, motion capture, lyrics, and metadata, to see
  whether machines can answer questions like:
</p>
<ul>
  <li>What is happening in the performance?</li>
  <li>Why is the performer moving this way?</li>
  <li>How does the audience react at certain points during the performance?</li>
</ul>

<p><strong>What is Music Performance?</strong></p>
<p>Let's just say that music performance is music in action. It‚Äôs when performers turn compositions, improvisations, or musical ideas into sound, movement, and 
  expression for an audience. Think of it as storytelling with sound, gestures, and emotion.</p>

<p><strong>What is Music Performance Context?</strong></p>
<p>Context is everything around the music that shapes its meaning: the venue, the audience, the gestures of performers, cultural rules, and even the placement of 
  instruments. It‚Äôs what makes the same piece of music feel different in a concert hall, a jazz club, or a folk festival.</p>

<p><strong>What is Music Understanding?</strong></p>
<p>Music understanding begins when we stop simply hearing sound around us and start listening with attention. 
  Hearing is passive and happens automatically. Listening is active and happens when we focus, follow, feel, and make sense of what unfolds in the music. 
  Understanding music isn‚Äôt about knowing fancy terminology or analyzing scores, but about noticing what the music is doing, how it moves, how it feels, and how our
  own experiences help us interpret it.
</p>
<p>People often imagine that only trained musicians or musicologists can ‚Äúunderstand‚Äù music, but everyone can! A person may not know what a motif or cadence is, 
  but they can still feel repetition, arrival, contrast, or surprise. They can follow a musical idea as it evolves, or sense the energy in a performer‚Äôs gesture. 
  Understanding music is less about naming things and more about experiencing how music flows and how it speaks.
</p>

<!-- TREE -->
<div class="tree">
  <ul>
    <li>
      <strong>Music Understanding</strong>
      <ul>
        <li>
          <strong>Listening (not just hearing!)</strong>
          <ul>
            <li>Attention + curiosity</li>
          </ul>
        </li>
        <li>
          <strong>Perception</strong>
          <ul>
            <li>How we group, follow, and sense musical ideas</li>
          </ul>
        </li>
        <li>
          <strong>Cognition</strong>
          <ul>
            <li>Memory, prediction, timing, reasoning</li>
          </ul>
        </li>
        <li>
          <strong>Embodiment</strong>
          <ul>
            <li>How our body feels, moves, and simulates the music</li>
          </ul>
        </li>
        <li>
          <strong>Emotion &amp; Experience</strong>
          <ul>
            <li>What the music makes us feel and imagine</li>
          </ul>
        </li>
        <li>
          <strong>Culture</strong>
          <ul>
            <li>What our background teaches us to notice and value</li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</div>

<p><strong>What is music information processing?</strong></p>
<p>Computational music information processing is about turning the physical experience of music into digital data, and then into forms of machine cognition, so that
  computers can help us explore, analyze, and understand music in new ways.</p>

<!-- Thebe Python cell -->
<div class="cell-block thebe-cell" data-executable="true" data-language="python">
<pre data-executable="true" data-language="python">
metadata = {
"tempo": 95,
"key": "A minor",
"instrumentation": ["violin", "laouto", "voice"],
}

question = "What is the key of the piece?"

if "key" in question.lower():
    answer = metadata["key"]
else:
    answer = "Not sure yet!"

answer
</pre>
</div>

<!-- INTERACTIVE MULTIMODALITY TEST -->
<div class="multimodal-test">
  <p><strong>Is my dataset multimodal?</strong></p>

  <p><em>Step 1 ‚Äî What task are you performing?</em></p>
  <select id="taskSelect">
    <option value="">-- Select a task --</option>
    <optgroup label="Categorization-oriented">
      <option>Emotion/Affect Recognition</option>
      <option>Genre Classification</option>
      <option>Auto-Tagging</option>
      <option>Musical Gesture Classification</option>
      <option>Singing Voice Analysis</option>
    </optgroup>
    <optgroup label="Time-dependent representation">
      <option>Source Separation</option>
      <option>Piano Tutoring</option>
      <option>Music Segmentation</option>
      <option>Music Structure Analysis</option>
      <option>Music Transcription</option>
      <option>Instrument Performance Analysis</option>
    </optgroup>
    <optgroup label="Music similarity-oriented">
      <option>Song Retrieval</option>
      <option>Music Exploration & Discovery</option>
    </optgroup>
    <optgroup label="Music Generation">
      <option>Music Generation</option>
    </optgroup>
    <optgroup label="Multi-task">
      <option>Multi-task MIR</option>
    </optgroup>
  </select>

  <p><em>Step 2 ‚Äî What data does your dataset contain?</em></p>
  <label><input type="checkbox" class="modality" value="audio"> Audio</label>
  <label><input type="checkbox" class="modality" value="lyrics"> Lyrics/Text</label>
  <label><input type="checkbox" class="modality" value="midi"> MIDI/Symbolic Score</label>
  <label><input type="checkbox" class="modality" value="video"> Video</label>
  <label><input type="checkbox" class="modality" value="motion"> Motion Capture / IMU / EMG</label>
  <label><input type="checkbox" class="modality" value="physio"> Physiology (ECG, GSR...)</label>
  <label><input type="checkbox" class="modality" value="annotations"> Human Annotations</label>
  <label><input type="checkbox" class="modality" value="images"> Images (covers, etc.)</label>

  <button id="checkButton">Check</button>
  <p id="result"></p>
</div>

  <script>
// --- Complementarity definitions ---
// A modality is complementary if it contributes NEW information to the task
// according to your description in the text.
const complementaryMap = {
  "Emotion/Affect Recognition": ["audio", "lyrics", "physio", "video", "annotations", "images"],
  "Genre Classification": ["audio", "lyrics", "images", "video", "annotations"],
  "Auto-Tagging": ["audio", "lyrics", "images", "annotations"],
  "Musical Gesture Classification": ["audio", "motion", "video", "annotations", "physio", "midi"],
  "Singing Voice Analysis": ["audio", "video", "annotations"],
  "Source Separation": ["audio", "midi", "annotations"],
  "Piano Tutoring": ["audio", "midi"],
  "Music Segmentation": ["audio", "midi", "annotations"],
  "Music Structure Analysis": ["audio", "midi", "annotations"],
  "Music Transcription": ["audio", "midi", "video", "annotations"],
  "Instrument Performance Analysis": ["audio", "motion", "midi", "annotations"],
  "Song Retrieval": ["audio", "lyrics", "annotations"],
  "Music Exploration & Discovery": ["audio", "lyrics", "images", "annotations"],
  "Music Generation": ["audio", "video", "lyrics", "images"],
  "Multi-task MIR": ["audio", "midi", "video", "motion", "lyrics", "physio", "annotations", "images"]
};

// --- Updated multimodality test logic ---
document.getElementById("checkButton").addEventListener("click", function () {
  const task = document.getElementById("taskSelect").value;
  const checked = [...document.querySelectorAll(".modality:checked")].map(x => x.value);

  const resultEl = document.getElementById("result");

  if (!task) {
    resultEl.innerText = "Please choose a task first.";
    return;
  }

  if (checked.length === 0) {
    resultEl.innerText = "Please select at least one type of data.";
    return;
  }

  // List of complementary modalities for this task
  const complementary = complementaryMap[task] || [];

  // Which of the checked modalities are complementary?
  const complementarySelected = checked.filter(m => complementary.includes(m));

  // Decide multimodality based on complementarity, not number of modalities
  if (complementarySelected.length >= 2) {
    resultEl.innerText =
      "‚úÖ YES ‚Äî your dataset is multimodal because the selected modalities provide complementary, task-relevant information.";
  } else if (complementarySelected.length === 1) {
    resultEl.innerText =
      "üü° Partially multimodal ‚Äî only one selected modality offers complementary information for this task. Additional complementary modalities may improve performance.";
  } else {
    resultEl.innerText =
      "‚ùå No ‚Äî the selected modalities do not provide complementary information for this task. The dataset is not multimodal under the task-based definition.";
  }
});
</script>


});

// Thebe configuration
thebe.config = {
  useBinder: true,
  binderOptions: {
    repo: "binder-examples/jupyter-stacks-datascience",
    ref: "master",
  }
};
thebe.bootstrap();
</script>

  <h2>Multimodality Guide</h2>
<p>This interactive tool helps you explore which music processing tasks you can perform with your dataset, or which data you need for a target task.</p>

<!-- --- Panel: Data ‚Üí Tasks --- -->
<div class="cell-block">
  <strong>Step 1 ‚Äî I have this data:</strong><br>
  <label><input type="checkbox" class="data-check" value="audio"> Audio</label><br>
  <label><input type="checkbox" class="data-check" value="lyrics"> Lyrics/Text</label><br>
  <label><input type="checkbox" class="data-check" value="midi"> MIDI/Symbolic Score</label><br>
  <label><input type="checkbox" class="data-check" value="video"> Video</label><br>
  <label><input type="checkbox" class="data-check" value="motion"> Motion Capture / IMU / EMG</label><br>
  <label><input type="checkbox" class="data-check" value="physio"> Physiology (ECG, GSR...)</label><br>
  <label><input type="checkbox" class="data-check" value="annotations"> Human Annotations</label><br>
  <label><input type="checkbox" class="data-check" value="images"> Images (covers, etc.)</label><br>
  <button id="dataToTaskBtn">Which tasks can I do?</button>
  <p id="dataToTaskResult" style="font-weight:bold; margin-top:10px;"></p>
</div>

<!-- --- Panel: Task ‚Üí Data --- -->
<div class="cell-block">
  <strong>Step 2 ‚Äî I want to perform this task:</strong><br>
  <select id="taskSelect2">
    <option value="">-- Select a task --</option>
    <option value="emotion">Emotion/Affect Recognition</option>
    <option value="genre">Genre Classification / Auto-Tagging</option>
    <option value="gesture">Musical Gesture Classification</option>
    <option value="singing">Singing Voice Analysis</option>
    <option value="source">Source Separation</option>
    <option value="piano">Piano Tutoring</option>
    <option value="segmentation">Music Segmentation / Structure Analysis</option>
    <option value="transcription">Music Transcription</option>
    <option value="instrument">Instrument Performance Analysis</option>
    <option value="song">Song Retrieval</option>
    <option value="explore">Music Exploration & Discovery</option>
    <option value="generation">Music Generation</option>
    <option value="multi">Multi-task MIR</option>
  </select>
  <button id="taskToDataBtn">Which data do I need?</button>
  <p id="taskToDataResult" style="font-weight:bold; margin-top:10px;"></p>
</div>

<script>
// --- Data ‚Üí Tasks mapping
const dataToTasksMap = {
  audio: ["Emotion/Affect Recognition", "Genre Classification / Auto-Tagging", "Source Separation", "Piano Tutoring", "Music Segmentation / Structure Analysis", "Music Transcription", "Song Retrieval", "Music Exploration & Discovery", "Music Generation"],
  lyrics: ["Emotion/Affect Recognition", "Genre Classification / Auto-Tagging", "Music Exploration & Discovery", "Music Generation"],
  midi: ["Source Separation", "Piano Tutoring", "Music Segmentation / Structure Analysis", "Music Transcription"],
  video: ["Musical Gesture Classification", "Singing Voice Analysis", "Music Generation", "Multi-task MIR"],
  motion: ["Musical Gesture Classification", "Instrument Performance Analysis", "Multi-task MIR"],
  physio: ["Emotion/Affect Recognition"],
  annotations: ["Emotion/Affect Recognition", "Genre Classification / Auto-Tagging", "Musical Gesture Classification", "Singing Voice Analysis", "Source Separation", "Music Segmentation / Structure Analysis", "Music Transcription", "Instrument Performance Analysis", "Song Retrieval", "Music Exploration & Discovery", "Music Generation", "Multi-task MIR"],
  images: ["Genre Classification / Auto-Tagging", "Music Generation"]
};

// --- Task ‚Üí Data mapping with explanations
const taskToDataMap = {
  "emotion": {
    audio: "Audio conveys acoustic cues of emotion.",
    lyrics: "Lyrics add semantic context for emotion recognition.",
    physio: "Physiological signals capture emotional responses.",
    annotations: "Annotations provide ground truth for training."
  },
  "genre": {
    audio: "Audio contains timbre, rhythm, and instrumentation information.",
    lyrics: "Lyrics can indicate genre or style.",
    images: "Album/cover images may provide genre clues.",
    annotations: "Annotations give supervised labels."
  },
  "gesture": {
    motion: "Motion capture or sensors provide performer movement data.",
    video: "Video provides visual cues for gesture classification.",
    audio: "Audio helps contextualize the gestures.",
    annotations: "Human labels improve training and evaluation."
  },
  "singing": {
    audio: "Audio provides vocal features such as pitch and timbre.",
    video: "Video can support singer analysis.",
    annotations: "Annotations give ground truth for singing traits."
  },
  "source": {
    audio: "Audio contains the sounds to separate.",
    midi: "MIDI provides note alignment to guide separation."
  },
  "piano": {
    audio: "Audio captures performance nuances.",
    midi: "MIDI gives symbolic representation of notes and timing."
  },
  "segmentation": {
    audio: "Audio provides sound cues for segment boundaries.",
    midi: "MIDI helps with timing and structure alignment."
  },
  "transcription": {
    audio: "Audio is the main signal for transcription.",
    midi: "MIDI gives symbolic reference for notes.",
    video: "Video can supplement note timing in some datasets."
  },
  "instrument": {
    audio: "Audio captures sonic nuances of performance.",
    motion: "Motion shows finger or body movements.",
    midi: "MIDI gives score information."
  },
  "song": {
    audio: "Audio is used for similarity matching.",
    lyrics: "Lyrics help for textual matching and search.",
    annotations: "Annotations help label songs accurately."
  },
  "explore": {
    audio: "Audio used in recommendation models.",
    lyrics: "Lyrics add semantic meaning.",
    annotations: "Annotations improve exploration metrics.",
    images: "Images may support content-based recommendations."
  },
  "generation": {
    audio: "Audio provides training material for generation.",
    video: "Video adds visual cues for generated content.",
    lyrics: "Text helps generate context-specific music."
  },
  "multi": {
    audio: "Audio is a core modality.",
    midi: "MIDI supports multiple tasks.",
    video: "Video enriches multimodal learning.",
    motion: "Motion data allows gesture-based tasks.",
    lyrics: "Lyrics/text support semantic understanding.",
    physio: "Physiology supports emotion tasks.",
    annotations: "Annotations provide ground truth.",
    images: "Images add additional context."
  }
};

// --- Event handlers
document.getElementById("dataToTaskBtn").addEventListener("click", () => {
  const checked = [...document.querySelectorAll(".data-check:checked")].map(c => c.value);
  if(checked.length === 0){
    document.getElementById("dataToTaskResult").innerText = "Please select at least one data type.";
    return;
  }

  let taskSet = new Set();
  checked.forEach(d => {
    if(dataToTasksMap[d]){
      dataToTasksMap[d].forEach(t => taskSet.add(t));
    }
  });

  const taskList = Array.from(taskSet).sort();
  document.getElementById("dataToTaskResult").innerHTML = `<strong>Possible tasks:</strong><br>${taskList.join("<br>")}`;
});

document.getElementById("taskToDataBtn").addEventListener("click", () => {
  const task = document.getElementById("taskSelect2").value;
  if(!task){
    document.getElementById("taskToDataResult").innerText = "Please select a task first.";
    return;
  }

  const dataMap = taskToDataMap[task];
  let html = "<strong>Recommended data for this task:</strong><br>";
  for(const [d, expl] of Object.entries(dataMap)){
    html += `<strong>${d.charAt(0).toUpperCase() + d.slice(1)}:</strong> ${expl}<br>`;
  }
  document.getElementById("taskToDataResult").innerHTML = html;
});
</script>

<h2>Data Management Ethics & Compliance Test</h2>
<p>This interactive test helps you assess whether your dataset management aligns with ethical and legal requirements.</p>

<div class="cell-block" id="ethics-test">
  <div id="questions-container"></div>
  <div id="current-question-container">
    <div id="current-question"></div>
    <button id="yes-btn">Yes</button>
    <button id="no-btn">No</button>
    <p id="feedback" style="margin-top:10px; font-weight:bold;"></p>
  </div>
  <p id="ethics-score" style="margin-top:15px; font-weight:bold; font-size:1.1rem;"></p>
</div>

<script>
// Define the decision tree
const ethicsFlow = {
  "Preliminary Planning": [
    { q: "Do you adhere to FAIR principles?", riskNo: "‚ö†Ô∏è Risk of data mismanagement" },
    { q: "Do you have clear research goals?", riskNo: "‚ö†Ô∏è Risk of misaligned objectives" },
    { q: "Is your data diverse?", riskNo: "‚ö†Ô∏è Risk of bias" }
  ],
  "Dataset Construction": [
    { q: "Are ethical storage options considered?", riskNo: "‚ö†Ô∏è Risk of data breach" },
    { q: "Is the dataset structure cohesive?", riskNo: "‚ö†Ô∏è Risk of poor usability" },
    { q: "Are multiple modalities integrated thoughtfully?", riskNo: "‚ö†Ô∏è Risk of missing insights" }
  ],
  "Data Analysis": [
    { q: "Is the dataset refined based on results?", riskNo: "‚ö†Ô∏è Risk of stagnation" },
    { q: "Is documentation clear?", riskNo: "‚ö†Ô∏è Risk of confusion" },
    { q: "Is analysis optimized for reproducibility?", riskNo: "‚ö†Ô∏è Risk of poor reproducibility" }
  ],
  "Publication": [
    { q: "Are findings accurately interpreted?", riskNo: "‚ö†Ô∏è Risk of false conclusions" },
    { q: "Is the report documented comprehensively?", riskNo: "‚ö†Ô∏è Risk of knowledge loss" }
  ],
  "Archiving": [
    { q: "Is the dataset accessible and reproducible?", riskNo: "‚ö†Ô∏è Risk of inaccessible/forgotten research", riskYes: "‚úÖ Enduring impact!" }
  ]
};

const phases = Object.keys(ethicsFlow);
let currentPhase = 0;
let currentQIndex = 0;
let yesCount = 0;
let noCount = 0;

const questionEl = document.getElementById("current-question");
const feedbackEl = document.getElementById("feedback");
const scoreEl = document.getElementById("ethics-score");
const questionsContainer = document.getElementById("questions-container");
const yesBtn = document.getElementById("yes-btn");
const noBtn = document.getElementById("no-btn");

// Show current question
function showQuestion() {
  if (currentPhase >= phases.length) {
    // End of test
    const total = yesCount + noCount;
    const percentage = ((yesCount / total) * 100).toFixed(0);
    scoreEl.innerText = `üìù Ethics Score: ${yesCount} / ${total} ( ${percentage}% positive adherence )`;
    questionEl.innerHTML = "<strong>All questions answered!</strong>";
    yesBtn.style.display = "none";
    noBtn.style.display = "none";
    feedbackEl.innerText = "";
    return;
  }

  const phase = phases[currentPhase];
  const qObj = ethicsFlow[phase][currentQIndex];
  questionEl.innerHTML = `<strong>[${phase}]</strong> ${qObj.q}`;
  feedbackEl.innerText = "";
}

// Handle Yes click
yesBtn.addEventListener("click", () => {
  const phase = phases[currentPhase];
  const qObj = ethicsFlow[phase][currentQIndex];
  yesCount++;

  const message = qObj.riskYes ? qObj.riskYes : "‚úÖ Great!";
  // Append the answered question and feedback to the container
  const div = document.createElement("div");
  div.innerHTML = `<strong>[${phase}]</strong> ${qObj.q} <br> <span style="color:green;">${message}</span>`;
  questionsContainer.appendChild(div);

  moveNext();
});

// Handle No click
noBtn.addEventListener("click", () => {
  const phase = phases[currentPhase];
  const qObj = ethicsFlow[phase][currentQIndex];
  noCount++;

  const message = qObj.riskNo;
  const div = document.createElement("div");
  div.innerHTML = `<strong>[${phase}]</strong> ${qObj.q} <br> <span style="color:red;">${message}</span>`;
  questionsContainer.appendChild(div);

  moveNext();
});

function moveNext() {
  const phase = phases[currentPhase];
  currentQIndex++;
  if (currentQIndex >= ethicsFlow[phase].length) {
    currentPhase++;
    currentQIndex = 0;
  }
  setTimeout(showQuestion, 400); // short delay for reading feedback
}

// Initialize first question
showQuestion();
</script>



</div>  
</body>
</html>
